---
layout:     post
title:      Generative AI Made Easy
date:       2023-11-30 15:26:00
summary:    Generative AI refers to a cutting-edge technology that enables machines to autonomously produce creative content, ranging from art and music to text and beyond. By harnessing advanced algorithms, it opens new frontiers in artificial intelligence, fostering creativity and generating novel solutions.
categories:  generative-ai
permalink:  /generative-ai
---

## Generative AI Made Easy

### Generative AI - How is it different?

![Image](/images/generative-ai/gen_ai_vs_ml_ai.gif "AI vs ML vs Generative AI")

<details>
    <summary>Notes</summary>
    <ul>
        <li>ğŸ”¹Artificial Intelligence: Create machines that can simulate human-like intelligence and behavio.</li>
        <li>ğŸ”¹Machine Learning: Learning from examples.</li>
        <li>ğŸ”¹Generative AI: Learning from examples to create new content. </li>
    </ul>
</details>


### Goal of Generative AI

![Image](/images/generative-ai/goal_of_gen_ai.gif "Goal of Generative AI")

<details>
    <summary>Notes</summary>
    <ul>
        <li>ğŸ”¹Goal: Generating New Content </li>
            <ul>
                <li>ğŸ”¸Instead of making predictions, Generative AI focuses on creating new data samples</li>
                <li>ğŸ”¸Examples</li>
                    <ul>
                        <li>âœ… Text Generation: Writing emails, essays & poems. Generating ideas. </li>
                        <li>âœ… Writing Code: Write, debug & analyze programs. </li>
                        <li>âœ… Images Generation: Creating paintings, drawings, or other forms of images. </li>
            </ul>
            </ul>
    </ul>
</details>


### Traditional ML

![Image](/images/generative-ai/traditional_ml.gif "Traditional ML")

<details>
    <summary>Notes</summary>
    <ul>
        <li>ğŸ”¹Traditional ML Models: </li>
            <ul>
                <li>ğŸ”¸Needed task specific training </li>
                <li>ğŸ”¸Multiple tasks â¡ï¸ Multiple trainings â¡ï¸ Multiple models</li>
            </ul>
    </ul>
</details>


### Generative AI - Foundation Models

![Image](/images/generative-ai/foundation_model.gif "Foundation Models")

<details>
    <summary>Notes</summary>
    <ul>
        <li>ğŸ”¹Foundation Models: Pre-Trained Multi Task Models </li>
            <ul>
                <li>ğŸ”¸Trained once (called pre-training) </li>
                <li>ğŸ”¸Same model can be used for multiple tasks </li>
                    <ul>
                        <li>âœ… Chatbot </li>
                        <li>âœ… Classification </li>
                        <li>âœ… Summarization </li>
                    </ul>
                <li>ğŸ”¸Some models are multi-modal as well: Text, video, audio, image... </li>
            </ul>
        <li>ğŸ”¹Large Language Models: Focused on text</li>
            <ul>
                <li>ğŸ”¸REMEMBER: Subset of Foundation Models </li>
                <li>ğŸ”¸Models that are trained on a lot of text to generate more text! </li>
            <ul>
    </ul>
</details>


### Generative AI vs Traditional ML

![Image](/images/generative-ai/traditional_model_vs_foundation_model.gif "Generative AI vs Traditional ML")



### An Overview of Generative AI Stack

![Image](/images/generative-ai/overview_of_gen_ai_stack.gif "Overview of Generative AI Stack")

<details>
    <summary>Notes</summary>
    <ul>
        <li>ğŸ”¹Applications </li>
            <ul>
                <li>ğŸ”¸ChatGPT </li>
                <li>ğŸ”¸DALLÂ·E </li>
                <li>ğŸ”¸Bing Search, â€¦ </li>
            </ul>
        <li>ğŸ”¹API: </li>
            <ul>
                <li>ğŸ”¸OpenAI API </li>
                <li>ğŸ”¸Azure OpenAI </li>
                <li>ğŸ”¸Google Cloud PaLM API, ... </li>
            <ul>
        <li>ğŸ”¹Foundation Models: </li>
            <ul>
                <li>ğŸ”¸OpenAI:  </li>
                    <ul>
                        <li>âœ… GPT (2, 3, 3.5, 4, 5, ..): Text, code and more.. </li>
                        <li>âœ… DALLÂ·E (1, 2, ...): Images </li>
                    </ul>
                <li>ğŸ”¸Open Source: </li>
                    <ul>
                        <li>âœ… OpenLLaMA (Meta): Generate text, images, and code </li>
                    </ul>
                <li>ğŸ”¸Other Vendors: Google PaLM, ... </li>
            <ul>
    </ul>
</details>



### Generative AI tries to Predict Next Word

![Image](/images/generative-ai/predict_netxt_word.gif "Predict Next Word")

<details>
    <summary>Notes</summary>
    <ul>
        <li>ğŸ”¹A key step in Generative AI For Text is predicting the next word </li>
        <li>ğŸ”¹During training, text based Generative AI models learn the probability that a word might occur in a specific context </li>
            <ul>
                <li>ğŸ”¸Context: "The cat sat on the" </li>
                <li>ğŸ”¸Example probabilities for next word: </li>
                    <ul>
                        <li>âœ… "mat": 0.4, "table": 0.2, "chair": 0.2, "moon": 0.1 </li>
                    </ul>
                <li>ğŸ”¸Model might choose the highest probable word and go on to predict subsequent words </li>
                <li>ğŸ”¸HOWEVER, you can control which of the words is chosen by controlling a few parameters! </li>
                    <ul>
                        <li>âœ… temperature, top_k, top_p etc! </li>
                    </ul>
            </ul>
    </ul>
</details>


### Generative AI Text - Uses Tokens instead of Words

![Image](/images/generative-ai/gen_ai_uses_token.gif "Generative AI Text - Uses Tokens instead of Words")

<details>
    <summary>Notes</summary>
    <ul>
        <li>ğŸ”¹TOKEN: A unit of text that might be a word </li>
            <ul>
                <li>ğŸ”¸BUT it can be a sub word, punctuation mark, a number, .. </li>
                <li>ğŸ”¸Why Tokens? </li>
                    <ul>
                        <li>âœ… Tokens are more consistent than words </li>
                        <ul>
                            <li>ğŸ“Œ Words can have multiple meanings, depending on the context </li>
                            <li>ğŸ“Œ "bank" might mean financial institution or a river bank </li>
                        </ul>
                        <li>âœ… Tokens are more consistent </li>
                        <ul>
                            <li>ğŸ“Œ Example tokens: bank_river, bank_financial or light_verb, light_noun, .. </li>
                        </ul>
                        <li>âœ… Tokens are smaller and more manageable </li>
                        <li>âœ… Tokens are more efficient to process </li>
                        <ul>
                            <li>ğŸ“Œ Because tokens are consistent, it easy for models to learn relationships and things like parts of speech </li>
                        </ul>
                    </ul>
            </ul>
        <li>ğŸ”¹Generative AI For Text Models: </li>
            <ul>
                <li>ğŸ”¸Understand relationships between Words Tokens </li>
                <li>ğŸ”¸Good at predicting Next Word Token! </li>
                <li>ğŸ”¸Have a token limit on context and generated text </li>
                    <ul>
                        <li>âœ… Example: 1,024 tokens or 4,096 tokens </li>
                    <ul>
            </ul>
    </ul>
</details>


### Making Generative AI More Creative

![Image](/images/generative-ai/temprature.gif "Temerature")

![Image](/images/generative-ai/top_p.gif "Top P")

![Image](/images/generative-ai/top_n.gif "Top N")

<details>
    <summary>Notes</summary>
    <ul>
        <li>ğŸ”¹A:0.4, B:0.2, C:0.1, D:0.05, E:0.02, F:0.01, .. Which of A, B, C, D, E, F should be chosen? </li>
            <ul>
                <li>ğŸ”¸Top-K: How many tokens should be considered? </li>
                    <ul>
                        <li>âœ… Specify the number of highest probability tokens to consider at each generation step </li>
                        <ul>
                            <li>ğŸ“Œ Example: top_k of 5 => next token is chosen from the top 5 most probable tokens </li>
                        </ul>
                    </ul>
                <li>ğŸ”¸Top-P: What is the (cumulative) probability limit ? </li>
                    <ul>
                        <li>âœ… Define the cumulative probability cutoff for selecting tokens </li>
                        <li>âœ… Lower value â¡ï¸ less random responses. Higher value â¡ï¸ more random responses. </li>
                        <ul>
                            <li>ğŸ“Œ Example: top_p value is 0.6 => Next token is either A or B </li>
                        </ul>
                    </ul>
                <li>ğŸ”¸Temperature: How random should be the output? </li>
                    <ul>
                        <li>âœ… Higher values â¡ï¸ more randomness and more creativity </li>
                        <li>âœ… Lower values â¡ï¸ lesser randomness</li>
                    </ul>
                <li>ğŸ”¸Example Scenarios: </li>
                    <ul>
                        <li>âœ… Find Capital City of India: use low values </li>
                        <li>âœ… Write a creative essay: use high values </li>
                    </ul>
            </ul>
    </ul>
</details>





### How are Generative AI Models Created?

![Image](/images/generative-ai/gen_ai_model_training.gif "Generative AI Model Training")

<details>
    <summary>Notes</summary>
    <ul>
        <li>ğŸ”¹Training needs huge volumes (Petabytes) of data </li>
        <li>ğŸ”¹Training process is very complex and needs huge amount of infrastructure </li>
            <ul>
                <li>ğŸ”¸Uses a combination of self supervised learning, SFT and RLHF </li>
            </ul>
        <li>ğŸ”¹Once trained, Generative AI Models can perform several tasks really well </li>
            <ul>
                <li>ğŸ”¸Based on Foundation Models and LLMs (Large Language Models) </li>
            </ul>
    </ul>
</details>

### Generative AI - Needs Huge Volumes of Data

<!-- ![Image](/images/generative-ai/gen_ai_model_training.gif "Temerature") -->

<details>
    <summary>Notes</summary>
    <ul>
        <li>ğŸ”¹Generative AI models: Statistical models that learn to generate new data by analyzing existing data </li>
            <ul>
                <li>ğŸ”¸More data analyzed => Better new data similar to existing data </li>
                <li>ğŸ”¸Example: GPT-3 model was trained on a dataset of 500 billion words of text </li>
            </ul>
        <li>ğŸ”¹Datasets used include: </li>
            <ul>
                <li>ğŸ”¸Images, text and code scraped from the open web: </li>
                <ul>
                    <li>âœ… Wikipedia </li>
                    <li>âœ… Books </li>
                    <li>âœ… Conversations </li>
                    <li>âœ… Open source code (syntax of programming languages and the semantics of code) </li>
                </ul>
            </ul>
    </ul>
</details>



### Generative AI - Uses Self Supervised Learning

![Image](/images/generative-ai/self_supervised_learning.gif "Self Supervised Learning")

<details>
    <summary>Notes</summary>
    <ul>
        <li>ğŸ”¹Self-supervised learning: Model learns from the data itself </li>
            <ul>
                <li>ğŸ”¸WITHOUT requiring explicit labels or annotations </li>
            </ul>
        <li>ğŸ”¹How does this work? </li>
            <ul>
                <li>ğŸ”¸Example for text model: </li>
                <ul>
                    <li>â¡ï¸ 1: Model tries to predict next word based on preceding words: </li>
                    <ul>
                        <li>âœ… Model is given an example sentence: "The sun is shining and the sky is ____." </li>
                        <li>âœ… Model predicts the missing word </li>
                    </ul>
                    <li>â¡ï¸ 2: Model's predicted word is compared to the actual word that comes next: </li>
                    <ul>
                        <li>âœ… Learns from its mistakes and adjusts its internal representation </li>
                        <ul>
                            <li>ğŸ“Œ Neural Networks, Loss Calculation, Backpropagation etc.. </li>
                        </ul>
                    </ul>
                    <li>â¡ï¸ 3: Repeated for all text from training dataset </li>
                </ul>
                <li>ğŸ”¸Model captures the relationships between words, contextual cues, and semantic meanings: </li>
                    <ul>
                        <li>âœ… If prompted with "The sun is shining and the sky is," the model might generate: </li>
                        <ul>
                            <li>ğŸ“Œ "The sun is shining and the sky is clear." </li>
                            <li>ğŸ“Œ "The sun is shining and the sky is blue." </li>
                            <li>ğŸ“Œ "The sun is shining and the sky is filled -- with fluffy clouds." </li>
                        </ul>
                    </ul>
            </ul>
    </ul>
</details>




### Generative AI Text - Uses SFT

![Image](/images/generative-ai/self_supervised_learning.gif "SFT")

<details>
    <summary>Notes</summary>
    <ul>
        <li>ğŸ”¹After basic training, Gen. AI Model can predict NEXT WORD in a sequence based on contextual information </li>
            <ul>
                <li>ğŸ”¸Given: "My favorite sport is" </li>
                    <ul>
                        <li>âœ… Model picks a probable word (basketball:20%, soccer:18%, cricket:10%) </li>
                    </ul>
                <li>ğŸ”¸Given: "A question?" </li>
                    <ul>
                        <li>âœ… Model might follow up with "Another Question?" </li>
                    </ul>
            </ul>
        <li>ğŸ”¹HOW to make a model to respond to questions with answers? </li>
            <ul>
                <li>ğŸ”¸Given a question, how to make the model give an answer </li>
                <li>ğŸ”¸Solution: Supervised Fine-Tuning </li>
            </ul>
        <li>ğŸ”¹Model is trained with Labeled Data </li>
            <ul>
                <li>ğŸ”¸1000s of Prompt and Ideal Response combinations </li>
                <li>ğŸ”¸Model learns to respond to a question with an answer </li>
                <li>ğŸ”¸Surprisingly less number of prompts add this capability </li>
            </ul>
    </ul>
</details>

### Generative AI Text - Uses RLHF

![Image](/images/generative-ai/rlhf.gif "RLHF")

<details>
    <summary>Notes</summary>
    <ul>
        <li>ğŸ”¹How to make the model understand human values and preferences? </li>
            <ul>
                <li>ğŸ”¸Models don't inherently understand human values, ethics, or preferences </li>
                <li>ğŸ”¸Models can sometimes generate content that is inappropriate, biased, or conflicts with human values </li>
                <li>ğŸ”¸How can we avoid this? </li>
                <li>ğŸ”¸Solution: Reinforcement Learning from Human Feedback (RLHF) </li>
            </ul>
        <li>ğŸ”¹STEPS: </li>
            <ul>
                <li>ğŸ”¸1: Create a Reward Model that understands human values and preferences </li>
                <li>ğŸ”¸2: Tune Generative AI Model using Reward Model </li>
            </ul>
    </ul>
</details>

### Generative AI Text - Uses RLHF - Step 1

<!-- ![Image](/images/generative-ai/self_supervised_learning.gif "RLHF") -->

<details>
    <summary>Notes</summary>
    <ul>
        <li>ğŸ”¹Goal: Build a Reward Model that understands human values and preferences </li>
        <li>ğŸ”¹How? </li>
            <ul>
                <li>ğŸ”¸1: Use Generative AI model to generate multiple responses for a prompt </li>
                <li>ğŸ”¸2: Diverse set of human evaluators rank the responses </li>
                <li>ğŸ”¸3: Tune Reward Model based on the responses from human evaluators </li>
                <li>ğŸ”¸Repeat 1, 2, 3 for thousands of prompts </li>
            </ul>
        <li>ğŸ”¹Result: Reward Model understands human values and preferences (generates a Reward)! </li>
            <ul>
                <li>ğŸ”¸It will be used later to tune the responses from the Generative AI model </li>
            </ul>
    </ul>
</details>



### Generative AI Text - Uses RLHF - Step 2

<!-- ![Image](/images/generative-ai/self_supervised_learning.gif "RLHF") -->

<details>
    <summary>Notes</summary>
    <ul>
        <li>ğŸ”¹Goal: Tune Generative AI Model using Reward Model </li>
        <li>ğŸ”¹How? </li>
            <ul>
                <li>ğŸ”¸1: Feed a prompt into Generative AI Model to generate a response </li>
                <li>ğŸ”¸2: Calculate Reward using Reward Model </li>
                <li>ğŸ”¸3: Tune Generative AI Model based on the evaluation </li>
                <li>ğŸ”¸Repeat 1, 2, 3 for millions of prompts (automated) </li>
            </ul>
        <li>ğŸ”¹Result: Generative AI Model understands human values and preferences! </li>
        <li>ğŸ”¹REMEMBER: You don't need to understand everything about SFT and RHLF </li>
    </ul>
</details>


### What are Embeddings?

![Image](/images/generative-ai/embeddings.gif "Embeddings")

<details>
    <summary>Notes</summary>
    <ul>
        <li>ğŸ”¹Embeddings: Vector representations of words in a high-dimensional space </li>
            <ul>
                <li>ğŸ”¸Captures semantic relationships and contextual information </li>
            </ul>
        <li>ğŸ”¹Example: You can use multiple dimensions to represent animals: </li>
            <ul>
                <li>ğŸ”¸Size: "small," "medium," or "large." </li>
                <li>ğŸ”¸Diet: "carnivore," "herbivore," or "omnivore." </li>
                <li>ğŸ”¸Habitat: "aquatic," "terrestrial," or "arboreal." </li>
                <li>ğŸ”¸Movement: "flying," "running," "swimming," or "crawling." </li>
            </ul>
    </ul>
</details>

### Exploring Embeddings with an Example

<!-- ![Image](/images/generative-ai/embeddings.gif "Embeddings") -->

<details>
    <summary>Notes</summary>
    <ul>
        <li>ğŸ”¹Shown here is an embedding of a single word </li>
            <ul>
                <li>ğŸ”¸OpenAI Embeddings API provides 1536-dimensional vector embeddings </li>
                <li>ğŸ”¸i.e. Each word is being looked at from 1536 different dimensions </li>
            </ul>
        <li>ğŸ”¹Widely used in natural language processing (NLP) tasks </li>
            <ul>
                <li>ğŸ”¸Text Similarity: Measure semantic similarity between texts </li>
                <li>ğŸ”¸Recommendation Systems: Recommend items based on user preferences </li>
                <li>ğŸ”¸Clustering: Group similar texts </li>
                <li>ğŸ”¸Outlier Detection: Find text that does not fit the group </li>
                <li>ğŸ”¸Example: Similarity Calculation </li>
                <ul>
                    <li>ğŸ”¸Given two sentences </li>
                    <ul>
                        <li>âœ… "The sun is shining brightly." and "Cats and dogs are popular pets." </li>
                    </ul>
                    <li>ğŸ”¸Calculate similarity between the sentence embeddings. </li>
                    <li>ğŸ”¸Higher similarity indicates semantic closeness. </li>
                </ul>
            </ul>
    </ul>
</details>




